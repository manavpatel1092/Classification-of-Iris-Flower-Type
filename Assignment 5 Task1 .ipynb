{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manav "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we need to run a multilayer perceptron (feed forward neural network) with two hidden layers and rectified linear nonlinearities on the iris dataset using the keras Sequential interface. Then using GridSearchCV with StratifiedShuffle Split we find the regularization strength and the number of hidden layers and use that to evaluate the model on the test set.\n",
    "\n",
    "The Iris Dataset has samples of three different types of iris flowers (Iris setosa, Iris virginica and Iris versicolor)with varying physical qualities and it consists of the following attributes:\n",
    "- Sepal Length\n",
    "- Sepal Width\n",
    "- Petal Length\n",
    "- Petal Width "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the IRIS dataset \n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mtrand.RandomState at 0x2fbe0ba0fc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Seeding \n",
    "np.random.RandomState(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into train and test sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#IMporting Keras and other required libraries for Sequential interface\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a function for making the model with 2 hidden layers and calculating its accuracy \n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def make_model(optimizer='adam', hidden_size=32):\n",
    "    model = Sequential([\n",
    "        Dense(hidden_size, input_dim = 4),\n",
    "        Activation('relu'),\n",
    "        Dense(hidden_size),\n",
    "        Activation('relu'),\n",
    "        Dense(3),\n",
    "        Activation('softmax')\n",
    "    ])\n",
    "        \n",
    "    model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using KerasClassifier for modelling purpose\n",
    "clf = KerasClassifier(make_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GridSearchCV for Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting the dataset usingStratifiedShuffle split to use with GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "sss.get_n_splits(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 2.5679 - acc: 0.3258\n",
      "23/23 [==============================] - 0s 2ms/step\n",
      "89/89 [==============================] - 0s 39us/step\n",
      "Epoch 1/1\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 1.5905 - acc: 0.2584\n",
      "23/23 [==============================] - 0s 2ms/step\n",
      "89/89 [==============================] - 0s 45us/step\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 1.4887 - acc: 0.3667\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "90/90 [==============================] - 0s 39us/step\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 1.5731 - acc: 0.3000\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "90/90 [==============================] - 0s 39us/step\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 3.0698 - acc: 0.3889\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "90/90 [==============================] - 0s 94us/step\n",
      "Epoch 1/1\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.2913 - acc: 0.2921\n",
      "23/23 [==============================] - 0s 5ms/step\n",
      "89/89 [==============================] - 0s 39us/step\n",
      "Epoch 1/1\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 2.9016 - acc: 0.3258\n",
      "23/23 [==============================] - 0s 6ms/step\n",
      "89/89 [==============================] - 0s 79us/step\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.1362 - acc: 0.3333\n",
      "22/22 [==============================] - 0s 9ms/step\n",
      "90/90 [==============================] - 0s 67us/step\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 1.3728 - acc: 0.3111\n",
      "22/22 [==============================] - 0s 7ms/step\n",
      "90/90 [==============================] - 0s 33us/step\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 1.1815 - acc: 0.2444\n",
      "22/22 [==============================] - 0s 8ms/step\n",
      "90/90 [==============================] - 0s 67us/step\n",
      "Epoch 1/1\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.6294 - acc: 0.3820\n",
      "23/23 [==============================] - 0s 8ms/step\n",
      "89/89 [==============================] - 0s 67us/step\n",
      "Epoch 1/1\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.2799 - acc: 0.2809\n",
      "23/23 [==============================] - 0s 8ms/step\n",
      "89/89 [==============================] - 0s 50us/step\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 1.3419 - acc: 0.3333\n",
      "22/22 [==============================] - 0s 10ms/step\n",
      "90/90 [==============================] - 0s 50us/step\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 1.2616 - acc: 0.2333\n",
      "22/22 [==============================] - 0s 14ms/step\n",
      "90/90 [==============================] - 0s 50us/step\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.8885 - acc: 0.2667\n",
      "22/22 [==============================] - 0s 16ms/step\n",
      "90/90 [==============================] - 0s 83us/step\n",
      "Epoch 1/3\n",
      "89/89 [==============================] - 1s 15ms/step - loss: 1.5488 - acc: 0.3258\n",
      "Epoch 2/3\n",
      "89/89 [==============================] - 0s 107us/step - loss: 1.4799 - acc: 0.3258\n",
      "Epoch 3/3\n",
      "89/89 [==============================] - 0s 112us/step - loss: 1.4279 - acc: 0.3034\n",
      "23/23 [==============================] - 0s 20ms/step\n",
      "89/89 [==============================] - 0s 67us/step\n",
      "Epoch 1/3\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.0079 - acc: 0.3933\n",
      "Epoch 2/3\n",
      "89/89 [==============================] - 0s 67us/step - loss: 0.9705 - acc: 0.4045\n",
      "Epoch 3/3\n",
      "89/89 [==============================] - 0s 112us/step - loss: 0.9390 - acc: 0.4719\n",
      "23/23 [==============================] - 0s 13ms/step\n",
      "89/89 [==============================] - 0s 67us/step\n",
      "Epoch 1/3\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.3862 - acc: 0.3333\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - 0s 100us/step - loss: 1.3127 - acc: 0.3333\n",
      "Epoch 3/3\n",
      "90/90 [==============================] - 0s 105us/step - loss: 1.2471 - acc: 0.3333\n",
      "22/22 [==============================] - 0s 15ms/step\n",
      "90/90 [==============================] - 0s 72us/step\n",
      "Epoch 1/3\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 1.2873 - acc: 0.3111\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - 0s 144us/step - loss: 1.2205 - acc: 0.3111\n",
      "Epoch 3/3\n",
      "90/90 [==============================] - 0s 100us/step - loss: 1.1797 - acc: 0.2444\n",
      "22/22 [==============================] - 0s 16ms/step\n",
      "90/90 [==============================] - 0s 50us/step\n",
      "Epoch 1/3\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.1827 - acc: 0.0222\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - 0s 89us/step - loss: 1.1499 - acc: 0.0444\n",
      "Epoch 3/3\n",
      "90/90 [==============================] - 0s 116us/step - loss: 1.1189 - acc: 0.0556\n",
      "22/22 [==============================] - 0s 17ms/step\n",
      "90/90 [==============================] - 0s 54us/step\n",
      "Epoch 1/3\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.8885 - acc: 0.3258\n",
      "Epoch 2/3\n",
      "89/89 [==============================] - 0s 112us/step - loss: 1.6566 - acc: 0.3258\n",
      "Epoch 3/3\n",
      "89/89 [==============================] - 0s 129us/step - loss: 1.4526 - acc: 0.3258\n",
      "23/23 [==============================] - 0s 15ms/step\n",
      "89/89 [==============================] - 0s 57us/step\n",
      "Epoch 1/3\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.6627 - acc: 0.3258\n",
      "Epoch 2/3\n",
      "89/89 [==============================] - 0s 85us/step - loss: 1.5513 - acc: 0.3258\n",
      "Epoch 3/3\n",
      "89/89 [==============================] - 0s 90us/step - loss: 1.4502 - acc: 0.3258\n",
      "23/23 [==============================] - 0s 14ms/step\n",
      "89/89 [==============================] - 0s 62us/step\n",
      "Epoch 1/3\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 1.0382 - acc: 0.3333\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - 0s 100us/step - loss: 0.9923 - acc: 0.3111\n",
      "Epoch 3/3\n",
      "90/90 [==============================] - 0s 111us/step - loss: 0.9517 - acc: 0.1000\n",
      "22/22 [==============================] - 1s 24ms/step\n",
      "90/90 [==============================] - 0s 100us/step\n",
      "Epoch 1/3\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.6762 - acc: 0.3000\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - 0s 89us/step - loss: 1.4514 - acc: 0.3000\n",
      "Epoch 3/3\n",
      "90/90 [==============================] - 0s 83us/step - loss: 1.2509 - acc: 0.3000\n",
      "22/22 [==============================] - 0s 17ms/step\n",
      "90/90 [==============================] - 0s 63us/step\n",
      "Epoch 1/3\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 1.1048 - acc: 0.5000\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - 0s 89us/step - loss: 1.0384 - acc: 0.7333\n",
      "Epoch 3/3\n",
      "90/90 [==============================] - 0s 89us/step - loss: 0.9898 - acc: 0.7333\n",
      "22/22 [==============================] - 0s 18ms/step\n",
      "90/90 [==============================] - 0s 61us/step\n",
      "Epoch 1/3\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 1.1607 - acc: 0.3146\n",
      "Epoch 2/3\n",
      "89/89 [==============================] - 0s 95us/step - loss: 1.0163 - acc: 0.4607\n",
      "Epoch 3/3\n",
      "89/89 [==============================] - 0s 135us/step - loss: 0.9331 - acc: 0.7079\n",
      "23/23 [==============================] - 0s 17ms/step\n",
      "89/89 [==============================] - 0s 62us/step\n",
      "Epoch 1/3\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.6279 - acc: 0.3258\n",
      "Epoch 2/3\n",
      "89/89 [==============================] - 0s 95us/step - loss: 1.3086 - acc: 0.3258\n",
      "Epoch 3/3\n",
      "89/89 [==============================] - 0s 90us/step - loss: 1.0854 - acc: 0.3258\n",
      "23/23 [==============================] - 0s 19ms/step\n",
      "89/89 [==============================] - 0s 56us/step\n",
      "Epoch 1/3\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.4902 - acc: 0.3000\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - 0s 100us/step - loss: 1.1963 - acc: 0.5333\n",
      "Epoch 3/3\n",
      "90/90 [==============================] - 0s 100us/step - loss: 1.0237 - acc: 0.3778\n",
      "22/22 [==============================] - 0s 20ms/step\n",
      "90/90 [==============================] - 0s 61us/step\n",
      "Epoch 1/3\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.3271 - acc: 0.2444\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - 0s 89us/step - loss: 1.0747 - acc: 0.3889\n",
      "Epoch 3/3\n",
      "90/90 [==============================] - 0s 100us/step - loss: 1.0433 - acc: 0.3889\n",
      "22/22 [==============================] - 0s 20ms/step\n",
      "90/90 [==============================] - 0s 55us/step\n",
      "Epoch 1/3\n",
      "90/90 [==============================] - 2s 25ms/step - loss: 1.1239 - acc: 0.3889\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - 0s 116us/step - loss: 0.9911 - acc: 0.3889\n",
      "Epoch 3/3\n",
      "90/90 [==============================] - 0s 111us/step - loss: 0.9107 - acc: 0.6333\n",
      "22/22 [==============================] - 1s 24ms/step\n",
      "90/90 [==============================] - 0s 67us/step\n",
      "Epoch 1/5\n",
      "89/89 [==============================] - 2s 18ms/step - loss: 1.9214 - acc: 0.2921\n",
      "Epoch 2/5\n",
      "89/89 [==============================] - 0s 95us/step - loss: 1.7745 - acc: 0.2921\n",
      "Epoch 3/5\n",
      "89/89 [==============================] - 0s 101us/step - loss: 1.6315 - acc: 0.2921\n",
      "Epoch 4/5\n",
      "89/89 [==============================] - 0s 107us/step - loss: 1.4892 - acc: 0.2921\n",
      "Epoch 5/5\n",
      "89/89 [==============================] - 0s 135us/step - loss: 1.3785 - acc: 0.2921\n",
      "23/23 [==============================] - 1s 26ms/step\n",
      "89/89 [==============================] - 0s 73us/step\n",
      "Epoch 1/5\n",
      "89/89 [==============================] - 2s 17ms/step - loss: 1.7356 - acc: 0.2809\n",
      "Epoch 2/5\n",
      "89/89 [==============================] - 0s 95us/step - loss: 1.6705 - acc: 0.2809\n",
      "Epoch 3/5\n",
      "89/89 [==============================] - 0s 151us/step - loss: 1.6019 - acc: 0.2809\n",
      "Epoch 4/5\n",
      "89/89 [==============================] - 0s 118us/step - loss: 1.5383 - acc: 0.2921\n",
      "Epoch 5/5\n",
      "89/89 [==============================] - 0s 101us/step - loss: 1.4786 - acc: 0.3371\n",
      "23/23 [==============================] - 1s 27ms/step\n",
      "89/89 [==============================] - 0s 62us/step\n",
      "Epoch 1/5\n",
      "90/90 [==============================] - 1s 15ms/step - loss: 1.1833 - acc: 0.3333\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 0s 94us/step - loss: 1.1431 - acc: 0.3333\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 0s 94us/step - loss: 1.1061 - acc: 0.3444\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 0s 89us/step - loss: 1.0717 - acc: 0.5667\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 0s 89us/step - loss: 1.0412 - acc: 0.6333\n",
      "22/22 [==============================] - 1s 23ms/step\n",
      "90/90 [==============================] - 0s 61us/step\n",
      "Epoch 1/5\n",
      "90/90 [==============================] - 1s 15ms/step - loss: 1.8900 - acc: 0.3000\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 0s 94us/step - loss: 1.7955 - acc: 0.3000\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 0s 94us/step - loss: 1.7057 - acc: 0.3000\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 0s 94us/step - loss: 1.6135 - acc: 0.3000\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 0s 105us/step - loss: 1.5442 - acc: 0.3000\n",
      "22/22 [==============================] - 1s 23ms/step\n",
      "90/90 [==============================] - 0s 61us/step\n",
      "Epoch 1/5\n",
      "90/90 [==============================] - 1s 16ms/step - loss: 1.4662 - acc: 0.0333\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 0s 94us/step - loss: 1.4224 - acc: 0.1222\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 0s 100us/step - loss: 1.3867 - acc: 0.2222\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 0s 78us/step - loss: 1.3514 - acc: 0.2667\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 0s 67us/step - loss: 1.3224 - acc: 0.2667\n",
      "22/22 [==============================] - 1s 30ms/step\n",
      "90/90 [==============================] - 0s 78us/step\n",
      "Epoch 1/5\n",
      "89/89 [==============================] - 2s 17ms/step - loss: 1.5710 - acc: 0.3820\n",
      "Epoch 2/5\n",
      "89/89 [==============================] - 0s 118us/step - loss: 1.3442 - acc: 0.3820\n",
      "Epoch 3/5\n",
      "89/89 [==============================] - 0s 101us/step - loss: 1.1658 - acc: 0.3820\n",
      "Epoch 4/5\n",
      "89/89 [==============================] - 0s 95us/step - loss: 1.0235 - acc: 0.4494\n",
      "Epoch 5/5\n",
      "89/89 [==============================] - 0s 107us/step - loss: 0.9166 - acc: 0.7079\n",
      "23/23 [==============================] - 1s 24ms/step\n",
      "89/89 [==============================] - 0s 73us/step\n",
      "Epoch 1/5\n",
      "89/89 [==============================] - 1s 17ms/step - loss: 1.2755 - acc: 0.4607\n",
      "Epoch 2/5\n",
      "89/89 [==============================] - 0s 95us/step - loss: 1.1741 - acc: 0.7191\n",
      "Epoch 3/5\n",
      "89/89 [==============================] - 0s 95us/step - loss: 1.0833 - acc: 0.7191\n",
      "Epoch 4/5\n",
      "89/89 [==============================] - 0s 112us/step - loss: 1.0119 - acc: 0.7191\n",
      "Epoch 5/5\n",
      "89/89 [==============================] - 0s 112us/step - loss: 0.9441 - acc: 0.7191\n",
      "23/23 [==============================] - 1s 25ms/step\n",
      "89/89 [==============================] - 0s 67us/step\n",
      "Epoch 1/5\n",
      "90/90 [==============================] - 2s 17ms/step - loss: 1.5887 - acc: 0.3667\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 0s 94us/step - loss: 1.4249 - acc: 0.3667\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 0s 100us/step - loss: 1.2858 - acc: 0.3667\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 0s 100us/step - loss: 1.1865 - acc: 0.3667\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 0s 111us/step - loss: 1.1056 - acc: 0.3667\n",
      "22/22 [==============================] - 1s 26ms/step\n",
      "90/90 [==============================] - 0s 67us/step\n",
      "Epoch 1/5\n",
      "90/90 [==============================] - 2s 17ms/step - loss: 1.7807 - acc: 0.0444\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 0s 100us/step - loss: 1.5405 - acc: 0.3889\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 0s 94us/step - loss: 1.3273 - acc: 0.3889\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 0s 94us/step - loss: 1.1353 - acc: 0.3889\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 0s 105us/step - loss: 1.0291 - acc: 0.3889\n",
      "22/22 [==============================] - 1s 27ms/step\n",
      "90/90 [==============================] - 0s 67us/step\n",
      "Epoch 1/5\n",
      "90/90 [==============================] - 2s 17ms/step - loss: 0.9946 - acc: 0.6667\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 0s 105us/step - loss: 0.9063 - acc: 0.7333\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 0s 105us/step - loss: 0.8490 - acc: 0.6778\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 0s 94us/step - loss: 0.8055 - acc: 0.7222\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 0s 105us/step - loss: 0.7653 - acc: 0.7333\n",
      "22/22 [==============================] - 1s 34ms/step\n",
      "90/90 [==============================] - 0s 59us/step\n",
      "Epoch 1/5\n",
      "89/89 [==============================] - 2s 22ms/step - loss: 1.0890 - acc: 0.4157\n",
      "Epoch 2/5\n",
      "89/89 [==============================] - 0s 107us/step - loss: 0.9877 - acc: 0.6517\n",
      "Epoch 3/5\n",
      "89/89 [==============================] - 0s 107us/step - loss: 0.9149 - acc: 0.7079\n",
      "Epoch 4/5\n",
      "89/89 [==============================] - 0s 107us/step - loss: 0.8603 - acc: 0.7079\n",
      "Epoch 5/5\n",
      "89/89 [==============================] - 0s 140us/step - loss: 0.8072 - acc: 0.7079\n",
      "23/23 [==============================] - 1s 28ms/step\n",
      "89/89 [==============================] - 0s 84us/step\n",
      "Epoch 1/5\n",
      "89/89 [==============================] - 2s 18ms/step - loss: 1.1032 - acc: 0.3933\n",
      "Epoch 2/5\n",
      "89/89 [==============================] - 0s 107us/step - loss: 0.9772 - acc: 0.3933\n",
      "Epoch 3/5\n",
      "89/89 [==============================] - 0s 107us/step - loss: 0.8852 - acc: 0.6067\n",
      "Epoch 4/5\n",
      "89/89 [==============================] - 0s 112us/step - loss: 0.7998 - acc: 0.7191\n",
      "Epoch 5/5\n",
      "89/89 [==============================] - 0s 118us/step - loss: 0.7286 - acc: 0.7191\n",
      "23/23 [==============================] - 1s 27ms/step\n",
      "89/89 [==============================] - 0s 67us/step\n",
      "Epoch 1/5\n",
      "90/90 [==============================] - 2s 19ms/step - loss: 1.2026 - acc: 0.3000\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 0s 111us/step - loss: 1.0831 - acc: 0.3333\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 0s 111us/step - loss: 1.0097 - acc: 0.5778\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 0s 116us/step - loss: 0.9747 - acc: 0.6333\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 0s 105us/step - loss: 0.9455 - acc: 0.6333\n",
      "22/22 [==============================] - 1s 29ms/step\n",
      "90/90 [==============================] - 0s 75us/step\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 2s 19ms/step - loss: 1.2907 - acc: 0.1778\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 0s 105us/step - loss: 1.1554 - acc: 0.3889\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 0s 105us/step - loss: 1.0793 - acc: 0.3889\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 0s 100us/step - loss: 1.0255 - acc: 0.3889\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 0s 105us/step - loss: 0.9729 - acc: 0.3889\n",
      "22/22 [==============================] - 1s 30ms/step\n",
      "90/90 [==============================] - 0s 68us/step\n",
      "Epoch 1/5\n",
      "90/90 [==============================] - 2s 19ms/step - loss: 1.1392 - acc: 0.3889\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 0s 105us/step - loss: 0.9727 - acc: 0.3889\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 0s 100us/step - loss: 0.8732 - acc: 0.5111\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 0s 100us/step - loss: 0.7984 - acc: 0.7333\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 0s 111us/step - loss: 0.7416 - acc: 0.7333\n",
      "22/22 [==============================] - 1s 31ms/step\n",
      "90/90 [==============================] - 0s 78us/step\n",
      "Epoch 1/5\n",
      "112/112 [==============================] - 2s 20ms/step - loss: 1.2147 - acc: 0.3661\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 0s 98us/step - loss: 1.0320 - acc: 0.5893\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 0s 98us/step - loss: 0.9527 - acc: 0.4375\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 0s 98us/step - loss: 0.9187 - acc: 0.4643\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 0s 102us/step - loss: 0.8577 - acc: 0.7500\n",
      "{'epochs': 5, 'hidden_size': 64}\n"
     ]
    }
   ],
   "source": [
    "#GridSearchCV for determining using StratifiedShuffleSplit \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'epochs' : [1, 3, 5], 'hidden_size':  [10, 32, 64]}\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid = param_grid, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function with the best parmeters obtained from GridSearchCV \n",
    "def make_model(optimizer='adam', hidden_size=64):\n",
    "    model = Sequential([\n",
    "        Dense(hidden_size, input_dim = 4),\n",
    "        Activation('relu'),\n",
    "        Dense(hidden_size),\n",
    "        Activation('relu'),\n",
    "        Dense(3),\n",
    "        Activation('softmax')\n",
    "    ])        \n",
    "    model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we used categorical_crossentropy as the loss factor we need to convert the target train and test sets from vectors \n",
    "#into a matrix \n",
    "num_classes = 3\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 12 samples\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 1.0078 - acc: 0.4600 - val_loss: 0.8995 - val_acc: 0.5833\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 699us/step - loss: 0.7233 - acc: 0.7800 - val_loss: 0.6539 - val_acc: 0.5833\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 674us/step - loss: 0.5702 - acc: 0.7500 - val_loss: 0.6031 - val_acc: 0.5833\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 848us/step - loss: 0.4875 - acc: 0.8100 - val_loss: 0.5537 - val_acc: 0.5833\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 694us/step - loss: 0.4008 - acc: 0.8300 - val_loss: 0.4181 - val_acc: 0.7500\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 699us/step - loss: 0.3588 - acc: 0.8700 - val_loss: 0.2917 - val_acc: 0.9167\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3117 - acc: 0.8900 - val_loss: 0.3569 - val_acc: 0.8333\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2730 - acc: 0.9500 - val_loss: 0.2907 - val_acc: 0.9167\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 978us/step - loss: 0.2437 - acc: 0.9600 - val_loss: 0.3029 - val_acc: 0.9167\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2236 - acc: 0.9600 - val_loss: 0.1830 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1952 - acc: 0.9800 - val_loss: 0.1703 - val_acc: 0.9167\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1784 - acc: 0.9600 - val_loss: 0.1819 - val_acc: 0.9167\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1562 - acc: 0.9600 - val_loss: 0.1686 - val_acc: 0.9167\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 908us/step - loss: 0.1732 - acc: 0.9400 - val_loss: 0.3691 - val_acc: 0.8333\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 943us/step - loss: 0.1464 - acc: 0.9500 - val_loss: 0.3194 - val_acc: 0.8333\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 948us/step - loss: 0.1514 - acc: 0.9400 - val_loss: 0.1921 - val_acc: 0.9167\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 893us/step - loss: 0.1140 - acc: 0.9900 - val_loss: 0.1508 - val_acc: 0.9167\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1027 - acc: 0.9800 - val_loss: 0.2416 - val_acc: 0.9167\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1163 - acc: 0.9800 - val_loss: 0.1431 - val_acc: 0.9167\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1397 - acc: 0.9500 - val_loss: 0.1249 - val_acc: 0.9167\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0976 - acc: 0.9700 - val_loss: 0.0977 - val_acc: 0.9167\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 943us/step - loss: 0.1045 - acc: 0.9600 - val_loss: 0.1831 - val_acc: 0.9167\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1217 - acc: 0.9700 - val_loss: 0.2485 - val_acc: 0.9167\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 988us/step - loss: 0.0891 - acc: 0.9900 - val_loss: 0.1689 - val_acc: 0.9167\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0788 - acc: 0.9800 - val_loss: 0.3075 - val_acc: 0.9167\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1084 - acc: 0.9500 - val_loss: 0.2195 - val_acc: 0.9167\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0657 - acc: 0.9800 - val_loss: 0.0605 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0692 - acc: 0.9800 - val_loss: 0.0977 - val_acc: 0.9167\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0744 - acc: 0.9500 - val_loss: 0.1405 - val_acc: 0.9167\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0639 - acc: 0.9700 - val_loss: 0.0948 - val_acc: 0.9167\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 933us/step - loss: 0.1144 - acc: 0.9600 - val_loss: 0.0965 - val_acc: 0.9167\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 953us/step - loss: 0.0654 - acc: 0.9800 - val_loss: 0.0827 - val_acc: 0.9167\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 963us/step - loss: 0.0629 - acc: 0.9800 - val_loss: 0.1098 - val_acc: 0.9167\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0532 - acc: 0.9900 - val_loss: 0.3256 - val_acc: 0.9167\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 963us/step - loss: 0.0720 - acc: 0.9800 - val_loss: 0.2036 - val_acc: 0.9167\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 978us/step - loss: 0.0627 - acc: 0.9800 - val_loss: 0.2377 - val_acc: 0.9167\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 953us/step - loss: 0.0744 - acc: 0.9700 - val_loss: 0.1678 - val_acc: 0.9167\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 998us/step - loss: 0.0702 - acc: 0.9600 - val_loss: 0.1453 - val_acc: 0.9167\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 908us/step - loss: 0.0499 - acc: 0.9900 - val_loss: 0.2550 - val_acc: 0.9167\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 898us/step - loss: 0.0789 - acc: 0.9600 - val_loss: 0.2302 - val_acc: 0.9167\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 968us/step - loss: 0.0644 - acc: 0.9600 - val_loss: 0.3297 - val_acc: 0.9167\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1024 - acc: 0.9600 - val_loss: 0.1291 - val_acc: 0.9167\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0546 - acc: 0.9700 - val_loss: 0.2589 - val_acc: 0.9167\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 958us/step - loss: 0.0858 - acc: 0.9600 - val_loss: 0.2635 - val_acc: 0.9167\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 933us/step - loss: 0.0625 - acc: 0.9700 - val_loss: 0.2626 - val_acc: 0.9167\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 943us/step - loss: 0.0904 - acc: 0.9700 - val_loss: 0.3337 - val_acc: 0.9167\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 928us/step - loss: 0.0632 - acc: 0.9700 - val_loss: 0.2550 - val_acc: 0.9167\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0437 - acc: 0.9900 - val_loss: 0.1418 - val_acc: 0.9167\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0412 - acc: 0.9900 - val_loss: 0.0991 - val_acc: 0.9167\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0460 - acc: 0.9900 - val_loss: 0.1862 - val_acc: 0.9167\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 978us/step - loss: 0.0658 - acc: 0.9800 - val_loss: 0.2205 - val_acc: 0.9167\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0544 - acc: 0.9900 - val_loss: 0.3601 - val_acc: 0.9167\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0581 - acc: 0.9800 - val_loss: 0.2208 - val_acc: 0.9167\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0673 - acc: 0.9700 - val_loss: 0.2122 - val_acc: 0.9167\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 878us/step - loss: 0.0516 - acc: 0.9800 - val_loss: 0.2121 - val_acc: 0.9167\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 853us/step - loss: 0.0478 - acc: 0.9900 - val_loss: 0.1198 - val_acc: 0.9167\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 848us/step - loss: 0.0569 - acc: 0.9800 - val_loss: 0.1849 - val_acc: 0.9167\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 798us/step - loss: 0.0340 - acc: 0.9900 - val_loss: 0.1320 - val_acc: 0.9167\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 978us/step - loss: 0.0395 - acc: 0.9800 - val_loss: 0.1601 - val_acc: 0.9167\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 808us/step - loss: 0.0487 - acc: 0.9700 - val_loss: 0.1214 - val_acc: 0.9167\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 853us/step - loss: 0.0353 - acc: 0.9900 - val_loss: 0.2239 - val_acc: 0.9167\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 828us/step - loss: 0.0393 - acc: 1.0000 - val_loss: 0.1890 - val_acc: 0.9167\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 858us/step - loss: 0.0643 - acc: 0.9700 - val_loss: 0.3003 - val_acc: 0.9167\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 853us/step - loss: 0.0510 - acc: 0.9800 - val_loss: 0.2821 - val_acc: 0.9167\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 813us/step - loss: 0.0429 - acc: 0.9700 - val_loss: 0.1763 - val_acc: 0.9167\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 803us/step - loss: 0.1305 - acc: 0.9500 - val_loss: 0.0741 - val_acc: 0.9167\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 868us/step - loss: 0.0657 - acc: 0.9600 - val_loss: 0.1337 - val_acc: 0.9167\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 828us/step - loss: 0.0327 - acc: 0.9900 - val_loss: 0.3009 - val_acc: 0.9167\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 903us/step - loss: 0.0653 - acc: 0.9700 - val_loss: 0.3152 - val_acc: 0.9167\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 833us/step - loss: 0.0430 - acc: 1.0000 - val_loss: 0.1826 - val_acc: 0.9167\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 873us/step - loss: 0.0300 - acc: 0.9900 - val_loss: 0.1381 - val_acc: 0.9167\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 828us/step - loss: 0.0345 - acc: 0.9900 - val_loss: 0.0931 - val_acc: 0.9167\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 838us/step - loss: 0.0280 - acc: 1.0000 - val_loss: 0.2609 - val_acc: 0.9167\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 803us/step - loss: 0.0348 - acc: 0.9900 - val_loss: 0.3020 - val_acc: 0.9167\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 893us/step - loss: 0.0353 - acc: 0.9900 - val_loss: 0.2406 - val_acc: 0.9167\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 808us/step - loss: 0.0351 - acc: 0.9900 - val_loss: 0.3015 - val_acc: 0.9167\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 818us/step - loss: 0.0740 - acc: 0.9500 - val_loss: 0.3272 - val_acc: 0.9167\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 818us/step - loss: 0.0389 - acc: 0.9800 - val_loss: 0.0645 - val_acc: 0.9167\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 868us/step - loss: 0.0551 - acc: 0.9700 - val_loss: 0.1692 - val_acc: 0.9167\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 823us/step - loss: 0.0497 - acc: 0.9600 - val_loss: 0.1015 - val_acc: 0.9167\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 798us/step - loss: 0.0557 - acc: 0.9800 - val_loss: 0.1251 - val_acc: 0.9167\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 853us/step - loss: 0.0410 - acc: 0.9700 - val_loss: 0.1208 - val_acc: 0.9167\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 823us/step - loss: 0.0387 - acc: 0.9800 - val_loss: 0.2405 - val_acc: 0.9167\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 888us/step - loss: 0.0400 - acc: 0.9800 - val_loss: 0.3185 - val_acc: 0.9167\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 833us/step - loss: 0.0314 - acc: 0.9900 - val_loss: 0.2178 - val_acc: 0.9167\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 853us/step - loss: 0.0372 - acc: 0.9700 - val_loss: 0.2864 - val_acc: 0.9167\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 853us/step - loss: 0.0292 - acc: 1.0000 - val_loss: 0.0858 - val_acc: 0.9167\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 903us/step - loss: 0.0676 - acc: 0.9700 - val_loss: 0.1049 - val_acc: 0.9167\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 958us/step - loss: 0.0317 - acc: 0.9800 - val_loss: 0.2376 - val_acc: 0.9167\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 898us/step - loss: 0.0234 - acc: 0.9900 - val_loss: 0.1648 - val_acc: 0.9167\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 858us/step - loss: 0.0650 - acc: 0.9700 - val_loss: 0.2133 - val_acc: 0.9167\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 888us/step - loss: 0.0338 - acc: 0.9900 - val_loss: 0.2064 - val_acc: 0.9167\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0325 - acc: 0.9800 - val_loss: 0.1670 - val_acc: 0.9167\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 818us/step - loss: 0.0350 - acc: 0.9800 - val_loss: 0.1186 - val_acc: 0.9167\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 0s 873us/step - loss: 0.0217 - acc: 1.0000 - val_loss: 0.3078 - val_acc: 0.9167\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0532 - acc: 0.9700 - val_loss: 0.3913 - val_acc: 0.9167\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0445 - acc: 0.9800 - val_loss: 0.1444 - val_acc: 0.9167\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0295 - acc: 0.9800 - val_loss: 0.2991 - val_acc: 0.9167\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 973us/step - loss: 0.0196 - acc: 1.0000 - val_loss: 0.1710 - val_acc: 0.9167\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 898us/step - loss: 0.0251 - acc: 0.9900 - val_loss: 0.3006 - val_acc: 0.9167\n",
      "\n",
      "Summary:\n",
      "\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_142 (Dense)            (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 4,675\n",
      "Trainable params: 4,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Fitting the tuned model using GridSearchCV on the train set and printing the model summary \n",
    "model = make_model()\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=100,\n",
    "          batch_size=3, validation_split=.1)\n",
    "print('\\nSummary:\\n\\n')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.202\n",
      "Test Accuracy: 0.947\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the tuned model on the test set \n",
    "score = model.evaluate(X_test, y_test, batch_size=3, verbose=0)\n",
    "\n",
    "print(\"\\nTest loss: {:.3f}\".format(score[0]))\n",
    "print(\"Test Accuracy: {:.3f}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 12 samples\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0255 - acc: 0.9900 - val_loss: 0.2716 - val_acc: 0.9167\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 200us/step - loss: 0.0210 - acc: 0.9900 - val_loss: 0.2453 - val_acc: 0.9167\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 225us/step - loss: 0.0201 - acc: 0.9900 - val_loss: 0.2200 - val_acc: 0.9167\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 180us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.1998 - val_acc: 0.9167\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 160us/step - loss: 0.0196 - acc: 1.0000 - val_loss: 0.1864 - val_acc: 0.9167\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 130us/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.1784 - val_acc: 0.9167\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 135us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.1754 - val_acc: 0.9167\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 150us/step - loss: 0.0209 - acc: 1.0000 - val_loss: 0.1757 - val_acc: 0.9167\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 150us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.1814 - val_acc: 0.9167\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 160us/step - loss: 0.0199 - acc: 1.0000 - val_loss: 0.1914 - val_acc: 0.9167\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 165us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.2025 - val_acc: 0.9167\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 105us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.2139 - val_acc: 0.9167\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 155us/step - loss: 0.0192 - acc: 1.0000 - val_loss: 0.2243 - val_acc: 0.9167\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 145us/step - loss: 0.0194 - acc: 0.9900 - val_loss: 0.2313 - val_acc: 0.9167\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 120us/step - loss: 0.0195 - acc: 0.9900 - val_loss: 0.2345 - val_acc: 0.9167\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 85us/step - loss: 0.0196 - acc: 0.9900 - val_loss: 0.2346 - val_acc: 0.9167\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 105us/step - loss: 0.0195 - acc: 0.9900 - val_loss: 0.2318 - val_acc: 0.9167\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 160us/step - loss: 0.0193 - acc: 0.9900 - val_loss: 0.2278 - val_acc: 0.9167\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 160us/step - loss: 0.0191 - acc: 0.9900 - val_loss: 0.2226 - val_acc: 0.9167\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 155us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.2171 - val_acc: 0.9167\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 135us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.2114 - val_acc: 0.9167\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 130us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.2059 - val_acc: 0.9167\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 170us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.1997 - val_acc: 0.9167\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 145us/step - loss: 0.0192 - acc: 1.0000 - val_loss: 0.1962 - val_acc: 0.9167\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 155us/step - loss: 0.0191 - acc: 1.0000 - val_loss: 0.1967 - val_acc: 0.9167\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 190us/step - loss: 0.0191 - acc: 1.0000 - val_loss: 0.1982 - val_acc: 0.9167\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 160us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.2024 - val_acc: 0.9167\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 140us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.2067 - val_acc: 0.9167\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 150us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.2122 - val_acc: 0.9167\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 175us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.2152 - val_acc: 0.9167\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 165us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.2157 - val_acc: 0.9167\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 165us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.2176 - val_acc: 0.9167\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 135us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.2197 - val_acc: 0.9167\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 165us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.2213 - val_acc: 0.9167\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 155us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.2204 - val_acc: 0.9167\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 120us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.2194 - val_acc: 0.9167\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 155us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.2199 - val_acc: 0.9167\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 115us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2218 - val_acc: 0.9167\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 140us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2239 - val_acc: 0.9167\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 145us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2245 - val_acc: 0.9167\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2237 - val_acc: 0.9167\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 180us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2231 - val_acc: 0.9167\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 150us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2223 - val_acc: 0.9167\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 130us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2213 - val_acc: 0.9167\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 155us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.2216 - val_acc: 0.9167\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 125us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.2220 - val_acc: 0.9167\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 140us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2214 - val_acc: 0.9167\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 125us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2224 - val_acc: 0.9167\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 170us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.2208 - val_acc: 0.9167\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 125us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.2176 - val_acc: 0.9167\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 135us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.2138 - val_acc: 0.9167\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 150us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.2104 - val_acc: 0.9167\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 130us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2080 - val_acc: 0.9167\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 135us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.2085 - val_acc: 0.9167\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 180us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.2094 - val_acc: 0.9167\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 125us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.2106 - val_acc: 0.9167\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 150us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.2117 - val_acc: 0.9167\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 135us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.2111 - val_acc: 0.9167\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 105us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.2130 - val_acc: 0.9167\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 155us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.2155 - val_acc: 0.9167\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 160us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.2160 - val_acc: 0.9167\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 120us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.2152 - val_acc: 0.9167\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 100us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.2166 - val_acc: 0.9167\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 160us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.2183 - val_acc: 0.9167\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 130us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.2199 - val_acc: 0.9167\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 140us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.2223 - val_acc: 0.9167\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 135us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.2238 - val_acc: 0.9167\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 140us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.2274 - val_acc: 0.9167\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 140us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.2293 - val_acc: 0.9167\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 130us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.2311 - val_acc: 0.9167\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 120us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.2320 - val_acc: 0.9167\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 130us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.2302 - val_acc: 0.9167\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 155us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.2287 - val_acc: 0.9167\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 155us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.2250 - val_acc: 0.9167\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 160us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.2221 - val_acc: 0.9167\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 140us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2185 - val_acc: 0.9167\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 120us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.2132 - val_acc: 0.9167\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 170us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.2101 - val_acc: 0.9167\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 135us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.2080 - val_acc: 0.9167\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 150us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.2085 - val_acc: 0.9167\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 175us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.2098 - val_acc: 0.9167\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 135us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.2129 - val_acc: 0.9167\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 155us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.2147 - val_acc: 0.9167\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 140us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.2173 - val_acc: 0.9167\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 135us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2220 - val_acc: 0.9167\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 140us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2258 - val_acc: 0.9167\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 110us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.2289 - val_acc: 0.9167\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 120us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.2322 - val_acc: 0.9167\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 125us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2341 - val_acc: 0.9167\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 125us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2344 - val_acc: 0.9167\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 165us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2349 - val_acc: 0.9167\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 170us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.2345 - val_acc: 0.9167\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 125us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.2339 - val_acc: 0.9167\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 165us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2315 - val_acc: 0.9167\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 0s 160us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.2300 - val_acc: 0.9167\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 120us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.2290 - val_acc: 0.9167\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 105us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.2264 - val_acc: 0.9167\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 0s 140us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.2272 - val_acc: 0.9167\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 125us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.2261 - val_acc: 0.9167\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 125us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.2261 - val_acc: 0.9167\n"
     ]
    }
   ],
   "source": [
    "#Fitting the tuned model on the training set \n",
    "history_callback = model.fit(X_train, y_train, batch_size=64,\n",
    "                             epochs=100, verbose=1, validation_split=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a function to plot both the models accuracy and loss \n",
    "def plot_history(logger):\n",
    "    df = pd.DataFrame(logger.history)\n",
    "    df[['acc', 'val_acc']].plot()\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    df[['loss', 'val_loss']].plot(linestyle='--', ax=plt.twinx())\n",
    "    plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Accuracy and Loss v/s epochs\n",
    "plot_history(history_callback)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
